<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_8dz3f8xsbjmu-7>li:before{content:"\0025cb  "}.lst-kix_8dz3f8xsbjmu-8>li:before{content:"\0025a0  "}ul.lst-kix_srs0kcpdvakp-3{list-style-type:none}.lst-kix_srs0kcpdvakp-0>li:before{content:"\0025cf  "}ul.lst-kix_srs0kcpdvakp-4{list-style-type:none}.lst-kix_8dz3f8xsbjmu-3>li:before{content:"\0025cf  "}.lst-kix_8dz3f8xsbjmu-4>li:before{content:"\0025cb  "}ul.lst-kix_srs0kcpdvakp-1{list-style-type:none}ul.lst-kix_srs0kcpdvakp-2{list-style-type:none}ul.lst-kix_srs0kcpdvakp-7{list-style-type:none}.lst-kix_srs0kcpdvakp-1>li:before{content:"\0025cb  "}.lst-kix_srs0kcpdvakp-2>li:before{content:"\0025a0  "}ul.lst-kix_srs0kcpdvakp-8{list-style-type:none}.lst-kix_8dz3f8xsbjmu-2>li:before{content:"\0025a0  "}.lst-kix_8dz3f8xsbjmu-6>li:before{content:"\0025cf  "}ul.lst-kix_srs0kcpdvakp-5{list-style-type:none}ul.lst-kix_srs0kcpdvakp-6{list-style-type:none}ul.lst-kix_srs0kcpdvakp-0{list-style-type:none}.lst-kix_8dz3f8xsbjmu-5>li:before{content:"\0025a0  "}ul.lst-kix_9gwh9zkr0yrx-3{list-style-type:none}ul.lst-kix_9gwh9zkr0yrx-4{list-style-type:none}.lst-kix_srs0kcpdvakp-7>li:before{content:"\0025cb  "}.lst-kix_srs0kcpdvakp-8>li:before{content:"\0025a0  "}ul.lst-kix_9gwh9zkr0yrx-5{list-style-type:none}ul.lst-kix_9gwh9zkr0yrx-6{list-style-type:none}ul.lst-kix_9gwh9zkr0yrx-7{list-style-type:none}ul.lst-kix_9gwh9zkr0yrx-8{list-style-type:none}.lst-kix_srs0kcpdvakp-5>li:before{content:"\0025a0  "}.lst-kix_srs0kcpdvakp-6>li:before{content:"\0025cf  "}.lst-kix_srs0kcpdvakp-3>li:before{content:"\0025cf  "}.lst-kix_srs0kcpdvakp-4>li:before{content:"\0025cb  "}ul.lst-kix_9gwh9zkr0yrx-0{list-style-type:none}ul.lst-kix_9gwh9zkr0yrx-1{list-style-type:none}ul.lst-kix_9gwh9zkr0yrx-2{list-style-type:none}.lst-kix_h7cte494mp4s-4>li:before{content:"\0025cb  "}.lst-kix_h7cte494mp4s-3>li:before{content:"\0025cf  "}.lst-kix_9gwh9zkr0yrx-8>li:before{content:"\0025a0  "}ul.lst-kix_w6way7a6c5pl-7{list-style-type:none}.lst-kix_h7cte494mp4s-2>li:before{content:"\0025a0  "}ul.lst-kix_w6way7a6c5pl-8{list-style-type:none}.lst-kix_h7cte494mp4s-5>li:before{content:"\0025a0  "}.lst-kix_h7cte494mp4s-6>li:before{content:"\0025cf  "}.lst-kix_9gwh9zkr0yrx-6>li:before{content:"\0025cf  "}.lst-kix_h7cte494mp4s-0>li:before{content:"\0025cf  "}.lst-kix_h7cte494mp4s-7>li:before{content:"\0025cb  "}.lst-kix_h7cte494mp4s-8>li:before{content:"\0025a0  "}.lst-kix_9gwh9zkr0yrx-5>li:before{content:"\0025a0  "}.lst-kix_9gwh9zkr0yrx-7>li:before{content:"\0025cb  "}.lst-kix_h7cte494mp4s-1>li:before{content:"\0025cb  "}.lst-kix_w6way7a6c5pl-5>li:before{content:"\0025a0  "}ul.lst-kix_h7cte494mp4s-8{list-style-type:none}ul.lst-kix_h7cte494mp4s-7{list-style-type:none}.lst-kix_w6way7a6c5pl-4>li:before{content:"\0025cb  "}ul.lst-kix_h7cte494mp4s-6{list-style-type:none}ul.lst-kix_h7cte494mp4s-5{list-style-type:none}.lst-kix_w6way7a6c5pl-3>li:before{content:"\0025cf  "}ul.lst-kix_h7cte494mp4s-4{list-style-type:none}ul.lst-kix_h7cte494mp4s-3{list-style-type:none}ul.lst-kix_h7cte494mp4s-2{list-style-type:none}ul.lst-kix_h7cte494mp4s-1{list-style-type:none}ul.lst-kix_h7cte494mp4s-0{list-style-type:none}.lst-kix_w6way7a6c5pl-2>li:before{content:"\0025a0  "}ul.lst-kix_w6way7a6c5pl-3{list-style-type:none}ul.lst-kix_w6way7a6c5pl-4{list-style-type:none}ul.lst-kix_w6way7a6c5pl-5{list-style-type:none}ul.lst-kix_w6way7a6c5pl-6{list-style-type:none}.lst-kix_w6way7a6c5pl-1>li:before{content:"\0025cb  "}ul.lst-kix_w6way7a6c5pl-0{list-style-type:none}.lst-kix_w6way7a6c5pl-0>li:before{content:"\0025cf  "}ul.lst-kix_w6way7a6c5pl-1{list-style-type:none}ul.lst-kix_w6way7a6c5pl-2{list-style-type:none}ul.lst-kix_8dz3f8xsbjmu-3{list-style-type:none}ul.lst-kix_8dz3f8xsbjmu-4{list-style-type:none}ul.lst-kix_8dz3f8xsbjmu-5{list-style-type:none}ul.lst-kix_8dz3f8xsbjmu-6{list-style-type:none}ul.lst-kix_8dz3f8xsbjmu-0{list-style-type:none}ul.lst-kix_8dz3f8xsbjmu-1{list-style-type:none}ul.lst-kix_8dz3f8xsbjmu-2{list-style-type:none}.lst-kix_8dz3f8xsbjmu-0>li:before{content:"\0025cf  "}ul.lst-kix_8dz3f8xsbjmu-7{list-style-type:none}.lst-kix_8dz3f8xsbjmu-1>li:before{content:"\0025cb  "}ul.lst-kix_8dz3f8xsbjmu-8{list-style-type:none}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_9gwh9zkr0yrx-2>li:before{content:"\0025a0  "}.lst-kix_9gwh9zkr0yrx-1>li:before{content:"\0025cb  "}.lst-kix_9gwh9zkr0yrx-3>li:before{content:"\0025cf  "}.lst-kix_9gwh9zkr0yrx-0>li:before{content:"\0025cf  "}.lst-kix_9gwh9zkr0yrx-4>li:before{content:"\0025cb  "}.lst-kix_w6way7a6c5pl-6>li:before{content:"\0025cf  "}.lst-kix_w6way7a6c5pl-7>li:before{content:"\0025cb  "}.lst-kix_w6way7a6c5pl-8>li:before{content:"\0025a0  "}ol{margin:0;padding:0}table td,table th{padding:0}.c1{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c16{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c14{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c8{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c6{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c5{padding-top:14pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c10{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c13{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c7{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c15{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c11{margin-left:72pt;padding-left:0pt}.c3{color:inherit;text-decoration:inherit}.c9{padding:0;margin:0}.c18{font-style:italic}.c17{height:16pt}.c12{font-weight:700}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c15 doc-content"><h1 class="c16" id="h.2ziyh5ic6d8a"><span class="c4">Final Project Planning Statement</span></h1><p class="c10"><span class="c0">Name: Ellis Brown</span></p><p class="c10"><span class="c0">Date: Friday, October 28, 2022</span></p><p class="c10"><span class="c0">Class: CLS - 161 : Intro to Digital Humanities, Tufts University</span></p><p class="c10"><span class="c0">Professor: Micah Saxton</span></p><p class="c10"><span>Portfolio Link: </span><span class="c7"><a class="c3" href="https://www.google.com/url?q=https://ellis-brown.github.io/CLS-161/&amp;sa=D&amp;source=editors&amp;ust=1667172404415704&amp;usg=AOvVaw2qLWvW2aiNM1RgRYayvg8W">https://ellis-brown.github.io/CLS-161/</a></span></p><h2 class="c6" id="h.g3tme2iz8h68"><span class="c8">Questions</span></h2><p class="c10"><span class="c0">Wikipedia is a big source of information for today&rsquo;s digital world. Google search queries are a common way for many people to validate information they read online, read the news, or learn more about their favorite topic. Wikipedia is often within the top few searches when someone is looking for some type of empirical information. Therefore, Wikipedia seems like a very interesting piece of online information that has a lot to reveal about the world. Due to the volume of data, and my own language skills, I will be restricting the focus to only the English wikipedia pages.<br><br>I describe a reference to be a URL that points from one wikipedia page to another. I would like to find out the most referenced pages of wikipedia from other pages of wikipedia. After determining the most referenced wikipedia pages, I would also like to read through a portion of one of the top referenced pages, and see if I find the information and their sources to be reputable. Wikipedia is built off of online contributions from community members, so this experiment will be interesting to find out the reputability of what may be considered to be one of the articles read by most wikipedia contributors.</span></p><p class="c2"><span class="c0"></span></p><p class="c10"><span class="c0">Specifically, I will attempt to answer the following questions:</span></p><p class="c2"><span class="c0"></span></p><ul class="c9 lst-kix_w6way7a6c5pl-0 start"><li class="c1 li-bullet-0"><span class="c0">What are the top 10 most referenced Wikipedia pages this year?</span></li><li class="c1 li-bullet-0"><span class="c0">What were the top 10 most referenced Wikipedia pages 5 years ago?</span></li><li class="c1 li-bullet-0"><span class="c0">How many internal links are there to the top 10 most referenced wikipedia pages this year?</span></li><li class="c1 li-bullet-0"><span class="c0">What percent of pages are referenced by less than 10 other pages?</span></li><li class="c1 li-bullet-0"><span class="c0">How many wikipedia pages were there in 2017? How many pages are there now?</span></li><li class="c1 li-bullet-0"><span class="c0">How reliable is Wikipedia as a source, from the perspective of the accuracy of the information quoted as compared to the original source, and how reliable are the original sources? Analysis will be done on a section of one of the most referenced wikipedia pages.</span></li></ul><h2 class="c6" id="h.m0h1p2rs6hw6"><span class="c8">Data Source</span></h2><p class="c10"><span class="c0">The source for the data will be 2 versions of Wikipedia, specifically a version sometime in 2017, and another version sometime in 2022. I aim to have a September version of each, if possible. </span></p><p class="c2"><span class="c0"></span></p><p class="c10"><span>Web scraping for a project like this is a common idea, saving the list of visited pages, parsing the served HTML pages for more links and ignoring external references, but wikipedia has </span><span class="c7"><a class="c3" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Wikipedia:Database_download%23Please_do_not_use_a_web_crawler&amp;sa=D&amp;source=editors&amp;ust=1667172404417509&amp;usg=AOvVaw10GWrDEMI3gELinhZWIxX7">specifically asked that people not do that</a></span><span class="c0">.<br></span></p><p class="c10"><span>The downloadable archives can be downloaded in multiple ways. For a more clear explanation, see the </span><span class="c7"><a class="c3" href="#h.rbgf9q61xp11">Download</a></span><span class="c0">&nbsp;section at the end of this document.</span></p><h2 class="c6" id="h.p2uaem5jkbv7"><span class="c8">Data Collection</span></h2><p class="c10"><span>Each backup of Wikipedia, compressed, is about 25GB of data. Therefore, I should be able to process each copy of the Wikipedia archive on my computer one at a time, but not both. The data structures I have chosen will not need to hold all data in memory during the process, but stream the data during analysis. The compressed data comes in about 500 files, so each compressed archive file can be opened, processed, and deleted. Therefore, the memory load on my computer is expected to be similar to the compressed form of the data with a small scaling factor, so about 200GB maximum. I have 219 GB of space available, so the whole process of downloading, then analyzing, then deleting the data can be done in memory on my local computer.<br><br>Wikipedia articles are not stored on their servers in html </span><span>format, but rather in a more memory-friendly format including an XML header. Information for help parsing those files can be found on the </span><span class="c7"><a class="c3" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Wikipedia:Database_download%23Database_schema&amp;sa=D&amp;source=editors&amp;ust=1667172404418306&amp;usg=AOvVaw3txK6uKbFp8lzMI87T9Wjm">wikipedia download page here</a></span><span>. Specifically, </span><span class="c7"><a class="c3" href="https://www.google.com/url?q=https://gitlab.com/tozd/go/mediawiki&amp;sa=D&amp;source=editors&amp;ust=1667172404418704&amp;usg=AOvVaw0saVEeEojyfeqjyQ331hyw">this package</a></span><span>&nbsp;</span><span>helps process wiki dumps. There is also </span><span class="c7"><a class="c3" href="https://www.google.com/url?q=https://github.com/napsternxg/WikiUtils&amp;sa=D&amp;source=editors&amp;ust=1667172404418942&amp;usg=AOvVaw2t6hTK6Dj8HJzykvqTV9PI">a package </a></span><span class="c0">written in Python for parsing through sql dumps, but I have not found if that will be useful yet. I plan to rewrite all parsing scripts in Python, but most of the work will not be parsing the data, but rather creating the graph of references, which can be done with minimal parsing of the XML headers. </span></p><p class="c2"><span class="c0"></span></p><p class="c10"><span>Through the above processes, I should be able to obtain a local copy of the full wikipedia archive from some particular year, and process the data afterwards </span></p><h2 class="c6" id="h.ckk14mlp1b00"><span class="c8">Data Processing</span></h2><p class="c2"><span class="c0"></span></p><p class="c10"><span>I plan to use a graph to represent the relationship between pages, using a one-way directed graph for references. </span><span class="c7"><a class="c3" href="https://www.google.com/url?q=https://towardsdatascience.com/how-to-visualize-social-network-with-graph-theory-4b2dc0c8a99f&amp;sa=D&amp;source=editors&amp;ust=1667172404419723&amp;usg=AOvVaw3oxSeS4l-ZffkVKp7r0sMd">This article</a></span><span class="c0">&nbsp;talks about how to represent social relationships as a graph, which can be thought of as equivalent to the page references. It also provides hints as to how to write the scripts in Python for the node processing, but I plan to use a slightly different approach. Graphs can be thought about as a network of connections, which is a perfect model for my attempt to represent references between pages. </span></p><p class="c2"><span class="c0"></span></p><p class="c10"><span>Due to the large amount of data being processed, I plan to use a python object with the following fields: { name: URL, &nbsp;references: list of URL referenced by this page }, which will be placed into a dict() using the URL as keys, and the list as a value. After generating this dictionary, each entry in the dictionary can be iterated through and a running list of the top X entries that have a property, such as longest list, will be tracked, and eventually reported. The usefulness of collecting this in a graph would be possible expansions of the project, where information relationships between pages can be represented in something </span><span class="c7"><a class="c3" href="https://www.google.com/url?q=https://www.highcharts.com/blog/tutorials/network-graph/&amp;sa=D&amp;source=editors&amp;ust=1667172404420287&amp;usg=AOvVaw0lv6--qD9J-UXZVFIbE3Ai">like this network graph.</a></span><span class="c0">&nbsp;Furthermore, collecting the data like this would be nice to publish alongside the results, for future use by other researchers. However, these ideas are outside the scope of the project.</span></p><p class="c2"><span class="c0"></span></p><p class="c10"><span>This data can be made smaller, by instead of making a list of references to other pages, there can be a count of pages who reference each page. This would be represented as a dict mapping {URL of page -&gt;count of references who reference the same page}. I can look into this if the memory becomes too large. Using a python dictionary is a very good choice for processing a large number of files, as the operation time to modify the dictionary is expected to be constant on average, as seen in the current implementation of </span><span class="c7"><a class="c3" href="https://www.google.com/url?q=https://wiki.python.org/moin/TimeComplexity&amp;sa=D&amp;source=editors&amp;ust=1667172404420994&amp;usg=AOvVaw2xfgfp6GxA1gi3FJ_vgTIu">CPython&rsquo;s documentation</a></span><span>. It is also clear that I should be adding to the end of the list fields, which is constant time, rather than inserting somewhere else - such as in order - to achieve the fastest runtime for my program. Runtime will be important, because uncompressing then streaming 25GB of compressed data twice (once for the 2017 archive, once for 2022)</span><span class="c0"><br><br>I will parse through the pages and store the data in a python dictionary on first pass. A second pass will be necessary to determine the most frequently visited pages.</span></p><h2 class="c6" id="h.8jbwvqa4h7b1"><span class="c8">Data Presentation</span></h2><p class="c10"><span class="c0">After completion of the data collection process, I will present my findings in the following ways</span></p><p class="c2"><span class="c0"></span></p><ul class="c9 lst-kix_srs0kcpdvakp-0 start"><li class="c1 li-bullet-0"><span class="c0">A list of the most referenced 10 pages for this year, and the number of references</span></li><li class="c1 li-bullet-0"><span class="c0">A list of the most visited 10 pages 5 years ago</span></li><li class="c1 li-bullet-0"><span class="c0">A pie chart showing the number of pages with less than 10 internal references vs. all pages in wikipedia</span></li><li class="c1 li-bullet-0"><span class="c0">A pie chart showing number of newly created pages in the last 5 years vs. all pages in wikipedia (stretch goal)</span></li></ul><ul class="c9 lst-kix_8dz3f8xsbjmu-0 start"><li class="c1 li-bullet-0"><span>A 1-4 paragraph summary of my findings on the trustworthiness of the information presented in one of the top 10 most referenced pages. </span></li></ul><h2 class="c6" id="h.51hi86e2wufq"><span class="c8">Other Relevant Information</span></h2><p class="c10"><span>Wikipedia provides metrics on page lookups. There is no better source of truth for that information than the service themselves.</span><span class="c12">&nbsp;However, this is not the same question. I am not looking to find the most visited page, but rather, what page is most referenced / linked to</span><span>. Therefore, I am not finding the most visited sight, but rather, the most referenced. It may be found that the most visited and most referenced go hand in hand, but this will not be the focus of my project. My analysis will build on the fact that the most highly referenced pages may not be visited much by users, but are certainly deemed worthy of being linked to by Wikipedia contributors, and therefore should have a high bar for correctness from within the internal contributor community. </span></p><h2 class="c6" id="h.3kpkoge221jn"><span class="c8">Inspiration</span></h2><p class="c10"><span>This project is similar to the digital historian lesson I completed, which involved data mining an internet archive, then finding data stored in the XML headers. Neither were things I had done before. While I will be finding data found in the bodies of the text during this project, rather than the headers, the ideas are similar. </span><span class="c7"><a class="c3" href="https://www.google.com/url?q=https://ellis-brown.github.io/CLS-161/lessons/data-mining/reflection.html&amp;sa=D&amp;source=editors&amp;ust=1667172404422728&amp;usg=AOvVaw2Dj9Q0QAx8qCWrUKGKr8_r">You can read my lesson here</a></span><span class="c0">. Also, my friend told me that wikipedia was only 2.5GB, so I thought it would be easy to work with the dataset. (He was wrong)</span></p><p class="c2"><span class="c0"></span></p><hr><h2 class="c6 c17" id="h.nwt48xmpoaht"><span class="c8"></span></h2><h2 class="c6" id="h.gh7dw4vkdkrq"><span class="c8">More technical details &amp; Design Considerations</span></h2><h4 class="c5" id="h.y6qw91wx9gw3"><span class="c14">URLs:</span></h4><p class="c10"><span>There is no restriction for having multiple names for one page, which would ruin the collection of data. However, since all files are provided and organized by wikipedia, they conform to wikipedia&rsquo;s naming conventions. I have read through the </span><span class="c7"><a class="c3" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Wikipedia:Article_titles&amp;sa=D&amp;source=editors&amp;ust=1667172404423795&amp;usg=AOvVaw0tqSuO1L8QHoLId8L43feK">article naming conventions here</a></span><span>, and can confirm that references to each page should be unique, and </span><span class="c7"><a class="c3" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Wikipedia:Disambiguation&amp;sa=D&amp;source=editors&amp;ust=1667172404424190&amp;usg=AOvVaw1cfe2I0JeJv1OMrigrna3f">non-ambiguous as specified here</a></span><span class="c0">. Things that share a name, such as &ldquo;Turkey&rdquo; (the country) and &ldquo;Turkey&rdquo; (the food) will have unique and consistent URLs for the purpose of my project. </span></p><p class="c2"><span class="c0"></span></p><h4 class="c5" id="h.rbgf9q61xp11"><span class="c14">Download</span></h4><p class="c10"><span>I will download everything using bittorrent, which allows for large downloads from a peer to peer network serving the files. I have started a torrent today using the </span><span class="c7"><a class="c3" href="https://www.google.com/url?q=https://www.imore.com/how-use-command-line-download-bittorrent-files-macos&amp;sa=D&amp;source=editors&amp;ust=1667172404425211&amp;usg=AOvVaw2b8ggyWWnP52Kze7gHsNrW">transmission command line tool for MacOS</a></span><span>, and can confirm that there are hosts who are willing to serve the needed </span><span class="c7"><a class="c3" href="https://www.google.com/url?q=https://meta.wikimedia.org/wiki/Data_dump_torrents%23English_Wikipedia&amp;sa=D&amp;source=editors&amp;ust=1667172404425622&amp;usg=AOvVaw2zC_ZhhVG1GT_FqFziAy-t">files listed here</a></span><span>. My backup plan in case I run into issues is creating a script to download through the </span><span class="c7"><a class="c3" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Wikipedia:Database_download%23Where_do_I_get_it?&amp;sa=D&amp;source=editors&amp;ust=1667172404426024&amp;usg=AOvVaw0nPHTu2jQAN5uRtGFWC5Hc">internet archive list</a></span><span>, or </span><span class="c7"><a class="c3" href="https://www.google.com/url?q=https://dumps.wikimedia.org/enwiki/latest/&amp;sa=D&amp;source=editors&amp;ust=1667172404426378&amp;usg=AOvVaw2-r4mo8JEDLyNjd9m062jE">from this apache server here</a></span><span class="c0">. The apache server does not serve the 2017 files, which would require me to change the scope of my project, but it is highly unlikely that I would need to switch my download source.</span></p><p class="c2"><span class="c0"></span></p><h4 class="c5" id="h.3zkyywfvh8qu"><span class="c14">Script &amp; Runtime Considerations</span></h4><p class="c2"><span class="c12 c13"></span></p><ul class="c9 lst-kix_9gwh9zkr0yrx-0 start"><li class="c1 li-bullet-0"><span>The concurrency will be handled using python&rsquo;s </span><span class="c7"><a class="c3" href="https://www.google.com/url?q=https://docs.python.org/3/library/multiprocessing.html&amp;sa=D&amp;source=editors&amp;ust=1667172404427203&amp;usg=AOvVaw2EG-eMVcxY4aybbIgnXkr3">multiprocessing module</a></span><span>, as multiple threads are limited in speed by </span><span class="c7"><a class="c3" href="https://www.google.com/url?q=https://wiki.python.org/moin/GlobalInterpreterLock&amp;sa=D&amp;source=editors&amp;ust=1667172404427466&amp;usg=AOvVaw0gsBRz62a8ogI3qozW1xTO">python&rsquo;s global interpreter lock</a></span><span class="c0">, meaning only a single thread can actually execute at a time.</span></li></ul><ul class="c9 lst-kix_9gwh9zkr0yrx-1 start"><li class="c10 c11 li-bullet-0"><span class="c0">&nbsp;Given that the data is only 25GB compressed per run (2017 vs 2022), I do not plan to spend time optimizing the process, but rather let the program run once overnight when tested.</span></li></ul><ul class="c9 lst-kix_9gwh9zkr0yrx-0"><li class="c1 li-bullet-0"><span>The data structure used to hold the data will be a dictionary, using a </span><span class="c7"><a class="c3" href="https://www.google.com/url?q=https://docs.python.org/3/library/multiprocessing.html%23sharing-state-between-processes&amp;sa=D&amp;source=editors&amp;ust=1667172404427919&amp;usg=AOvVaw05nYO3pyB-PdkRXJydAo-3">Manager</a></span><span class="c0">&nbsp;to promise correctness due to correct access, imported from the multiprocessing module.</span></li><li class="c1 li-bullet-0"><span class="c0">A log file will be implemented to track all errors. </span></li><li class="c1 li-bullet-0"><span class="c0">The main python script will uncompress and unzip one file at a time, then parse through the unzipped files in parallel using a worker pool of 32 threads to concurrently parse the files. </span></li></ul><ul class="c9 lst-kix_9gwh9zkr0yrx-1 start"><li class="c10 c11 li-bullet-0"><span>Development will be done locally on my computer - a M1 macbook Pro - which has 10 single-threaded cores. I expect the bottleneck to not be compute cycles, but rather </span><span>reads</span><span class="c0">&nbsp;from disk as I will be streaming data. Having data streaming as the bottleneck is unavoidable, which is why more threads will be desired for this inherently parallel process. </span></li></ul><ul class="c9 lst-kix_9gwh9zkr0yrx-0"><li class="c1 li-bullet-0"><span class="c0">After completion of the parsing step, the original compressed archive file will be deleted. After all files have been parsed, a local memory copy of the dictionary will exist, which will be written to the disk for further inspection. </span></li></ul><ul class="c9 lst-kix_9gwh9zkr0yrx-1 start"><li class="c10 c11 li-bullet-0"><span class="c0">This will live in RAM ( or more-likely SWP space) until completion, for simplicity. I will consider using an incrementally updating database-style approach for this dictionary, but I believe that itself would be a bottleneck as the program progresses - further research is required. I am not familiar with how to improve this experience without implementing or adding an actual database to my project, which I have decided is outside the scope for this project.</span></li></ul><p class="c2"><span class="c0"></span></p><h4 class="c5" id="h.hhfagz4f59l0"><span class="c14">Possible Problems &amp; Mitigation</span></h4><p class="c10"><span>If I were to find out that the decompressed files are too large and make this project unable to be run on my local computer, I may either change the scope of my data collection to a sub-section of wikipedia, which is </span><span class="c7"><a class="c3" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Wikipedia:Database_download%23Where_do_I_get_it?&amp;sa=D&amp;source=editors&amp;ust=1667172404429044&amp;usg=AOvVaw00XDALbwHXKPk36QODuHF8">briefly referenced here</a></span><span class="c0">. Since I do not anticipate this to be a problem, I have not looked into this further yet. </span></p><p class="c2"><span class="c0"></span></p><p class="c10"><span>I have also looked into the possibility of my internet provider enforcing a </span><span class="c18">monthly-limit </span><span class="c0">on the amount of data that can be downloaded. My household is limited to 1TB per month. My alternative is using the Tufts internet over ethernet or wifi which has up to 1GB/s internet speed. (Earlier I noted that I would probably be limited by my download speed - 100 MB/s - but this would no longer be the case). I expect a download to complete in 2 hours. I am aware that Tufts has 1GB/s of internet on their fiber optic cable to certain buildings, so I will probably attempt the transfer from Halligan or the Joyce Cummings Center. </span></p><p class="c2"><span class="c0"></span></p><p class="c10"><span class="c0">It may turn out that the speed in which I am able to process data is too slow to complete this project &nbsp;in a reasonable amount of time. I considered using C++ for the speed. However, since the course was taught in Python3, I will use Python instead. If the speed proves to be a problem - which I do not expect it to be - I will first look into the script bottlenecks, and then I will change the scope of my project to include less data. See the first paragraph of this section for an explanation of how that may be done.</span></p><p class="c2"><span class="c0"></span></p><h4 class="c5" id="h.uf3a6nhdt6dv"><span class="c14">NGrams (not in scope)</span></h4><p class="c10"><span>I was also considering creating an N-Gram for the wikipedia article bodies, but the size of the data far exceeds what I can run locally. I do have 4 Dell Optiplex computers each with 200GB of space, but my calculations for a 2-gram of all uncompressed wikipedia pages will far exceed that, and will also involve code to query 4 different machines, each running and serving information over an open port, which would be outside the scope of this course. A 1-gram would be no fun. I have looked into the memory impact of storing N-grams in python to come to this conclusion by briefly reading through </span><span class="c7"><a class="c3" href="https://www.google.com/url?q=https://www.analyticsvidhya.com/blog/2021/09/what-are-n-grams-and-how-to-implement-them-in-python/&amp;sa=D&amp;source=editors&amp;ust=1667172404430245&amp;usg=AOvVaw2Nq7lzgMqhmxQh2uDAbW80">this python lesson on implementing ngrams</a></span><span>&nbsp;using tools we have partly covered in class.</span></p></body></html>