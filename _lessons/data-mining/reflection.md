# Programming Historian Lesson 1

Lesson completed: [Data Mining the Internet Archive Collection](https://programminghistorian.org/en/lessons/data-mining-the-internet-archive)

## Reflection on learning:

During this lesson, I had to work with a few libraries I was unfamiliar with. I worked with
`internetarchive`, `wordcloud`, and `pymarc` for the first time. The most interesting was internetarchive, which introduced the idea of searching an online archive for information, and allowed the script to locate the URL names of files, which could then be downloaded directly using the built in library. Also involved was a recommended timeout about slowing down requests to the web server. I am surprised that this timeout is implemented at the application layer of the program, rather than as part of the networking protocol, but I understand that a failure in the networking protocol from packet congestion would lead to a failure at the application level, so I guess it makes sense. I was curious about experimenting with the times for the timeout (0.001s vs 0.05s vs 0.5s vs 1s [recommend]),
but I did not get the chance to do this, because I introduced a caching system to prevent redownloads. Also, download is slow and depends on the network and the computer doing so, which impose multiple limitations at different points in time. However, these limitations are technical, but the main problems occur from the way in which this data collection is done.

Working with data collected over the years is not easy, data collection and processing is inconsistent, data cleaning is hard, and some metrics may not be recorded or lost. Firstly, the internet archive search generated 9769 items to be processed. Of these, 1 URL was not valid and could not be found on the server, which either implies a broken record, or that `_marc.xml` file we were interested in was not collected. For a complete investigation of this topic, one could reason that some archives were lost over the years, and 1/9769 missing was not relevant, but a more complete investigation into why this was is recommended. Secondly, the location data using the suggested schema could be scraped from 6947 out of 9769 data points. This is __far too low_, as its quite realistic to assume that the missing data could have been sourced from the same location, and the correlation between the location and mismatched or missing scheme data for location is high. This could greatly affect the integrity of the results. To account for this, either one must manually check all missing archives of 2822, which would be a tedious process, or look into patterns in the collected `_marc.xml` files. 

Cleaning the data turned out to be a large challenge as well. As this was not the focus of this module, I did not invest the needed time to perfect and fix the issue, but something clear came up within the data collection: Locations were not named consistently. We had "Boston", "Mass", "MA", "Boston, Mass" as all different locations recorded for archive records, but may or may not have referred to the real location. As a result, one goal would be to raise or lower the specificity to that of the same level, such as the state, or city level. However, it is not possible to make "Mass" more specifc, as the location in Mass is unclear. This points to the conclusion that __data collected over time may be missing context, and a perfect record of what happens is not always obtainable__. The data also contained extra characters, trailing and leading spaces, which I removed using regular expressions. Overall, however, the cleaning of the data for statistical assesment was inadequeate, due to the schemas missing for 2822 documents in the pattern we expected, and the inconsistency with the data we collected.

If I were to be adding more time spent on this project, I would make sure the data collected and gathered was all accounted for, errors in finding files were investigated, and all data processed was __manually__ checked at least once for consistency. While an extremely boring task, the __vast ways that similar data is presented needs to be accounted for, in order to prevent introducing bias from the perspective of the programmer due to a lack of understanding the dataset__. If I had choosen to bring everything to the state level, that introduces my bias that the location within the state is less of interest, and affects my conclusions and claims as well. Ultimately, it comes down to the person doing research to make the important decisions related to their claims and analysis. Anyone (even an AI) can build a web crawler on an archive, with enough time. The true challenge is making good use of that collected data.